---
title: "Buscar y recuperar datos del Global Biodiverity Information Facilty (GBIF) utilizando R"
subtitle: Primer Seminario Grupo Ecoinformática de la AEET
author: "Carlos Lara Romero (carlos.lara.romero@gmail.com)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true #table of content ture
    toc_depth: 2 ## upto three depths of headings
    theme: united ### many options for theme
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

*rgbif* es un paquete de R para buscar y recuperar datos del Global Biodiverity Information Facilty (GBIF).

https://docs.ropensci.org/rgbif/
https://github.com/ropensci/rgbif/

## 1. Preparación sesión

Instalar rgbif desde CRAN

```{r, eval=FALSE, message=FALSE, results=FALSE}
install.packages("rgbif")
```

O instalar la versión de desarrollo desde GitHub

```{r, eval=FALSE, message=FALSE, results=FALSE}
remotes::install_github("ropensci/rgbif")
```

Cargar rgbif en la consola de R

```{r}
library("rgbif")
```


## 2. Búsqueda de nombres de taxones

```{r}
taxrank()
```

La nomenclatura científica en gbif es una maraña de nombres aceptados, nombres antiguos y sinonimias. 

La función *name_lookup()* realiza una búsqueda que abarca el nombre científico y vernáculo, la descripción de la especie, la distribución y la clasificación completa en todos los usos del nombre de todas o algunas listas. Los resultados se ordenan por relevancia, ya que esta búsqueda suele devolver muchos resultados.

Búsqueda de una especie
```{r}
tax <- name_lookup(query = 'Cedrus atlantica', rank="species")
names(tax)
tax$meta
tax$data
```


```{r}

head(tax$data) 
```

Búsqueda de un género

```{r}
cedrus <- name_lookup(query = 'Cedrus', rank="genus")
```


```{r, echo=FALSE}
cedrus$data
```

Para ayudarnos, *rgbif* cuenta con la función *name_backbone* que permite buscar nombres en la taxonomía de GBIF. La función devuelve un data.frame para la coincidencia de taxones sugerida. Un solo taxón con muchas columnas.

```{r}
tax.backbone<-name_backbone(name='Cedrus atlantica', rank='species', kingdom='plants')
tax.backbone
key<-tax.backbone$usageKey
```

Una estrategia útil es comprobar el listado de nombres aceptados en GBIF y usar su identificador único (key) para realizar la búsqueda de presencias. Para ello podemos usar la funicón *name_suggest*. Esta función nos proporciona un servicio rápido y sencillo que devuelve hasta 20 usos del nombre haciendo coincidir el prefijo con el nombre científico. Los resultados se ordenan por relevancia.

```{r}
tax.suggest <- name_suggest(q="Cedrus atlantica", rank='species')
tax.suggest$data
key2<-tax.suggest$data$key[1]
```

La función *occ_count* permite estimar la cantidad de ocurrencia. Es muy útil en el proceso de diseño de las descargas de GBIF. 

```{r}
occ_count(taxonKey=key)
occ_count(taxonKey=key2)
```

## 3. Búsqueda y descarga de ocurrencias

La función *Occ_search()* permite buscar ocurrencias de GBIF. Por defecto se descargan más de 80 columnas de información. Accedemos a los datos a través de la ranura *data*.

```{r}
cedrus<-occ_search(taxonKey=key, limit=500) #Limitamos la descarga a 500 datos para agilizar la descarga.
cedrus$meta
dim(cedrus$data)
head(cedrus$data, n=10L)
```

En el siguiente enlace puedes consultar una referencia fácil de leer de los términos recomendados actualmente  por elGrupo de Mantenimiento de Darwin Core.

https://dwc.tdwg.org/terms/

Podemos seleccionar los campos que deseamos descargar

```{r}
campos<-c("key","scientificName", "decimalLatitude","decimalLongitude", 
          "issues", "datasetKey", "basisOfRecord","year")
cedrus<-occ_search(taxonKey =  key, fields= campos, limit = 5000)
dim(cedrus$data)
names(cedrus$data)
```


```{r}
head(cedrus$data)
```

Visualizar los datos en un mapa es sencillo

```{r, warning=FALSE, message=FALSE}
#Mapa con ggplot2
library(ggplot2)
world = map_data("world") #library maps

ggplot(world, aes(long, lat)) +
geom_polygon(aes(group = group), fill = "white", 
              color = "gray40", size = .2)+
geom_jitter(data = cedrus$data,
aes(decimalLongitude, decimalLatitude), alpha=0.6, 
             size = 2, color = "red")
```

*occ_search()* permite vectorizar los argumentos para hacer descargas múltiples. El resultado es una lista anidada.

```{r}
occ <- occ_search(scientificName = c("Cedrus atlantica", "Eucalyptus nitens"),
                  fields= campos,
                  limit = 500,
                  hasCoordinate = TRUE)
is.list(occ)
names(occ)
head(occ$`Cedrus atlantica`$data)
head(occ$`Eucalyptus nitens`$data)

occ.df<-rbind(occ$`Cedrus atlantica`$data, occ$`Eucalyptus nitens`$data) #Sólo funciona si comparten los mismos campos
occ.df
dim(occ.df)
```


Se puede utilizar un bucle para descargar un archivo por cada especie. Luego podemos unirlos en un único data.frame. Esta manera de proceder permite generar un código más reproducible. Además, tiene la ventaja de que a la vez podemos descargar la cita y almacenarla en una carpeta específica. GBIF exige que se cite escrupulosamente los datos de presencia. 

[https://www.gbif.org/citation-guidelines]

```{r,message=FALSE}
path<-"outputs/data/rgbif/occ_search/"
dir.create(path)
splist<-c("Cedrus atlantica", "Eucalyptus nitens")

for (i in splist){
dat.search<- occ_search(scientificName=i,fields=campos, limit=50)
dat<- dat.search$data
saveRDS(dat, paste0(path,i,"_down.Rdata"))
cit<-gbif_citation(dat.search)
saveRDS(cit, paste0(path,i,"_citation",".Rdata"))
}
```

Podríamos modificar el bucle anterior para construir un único arreglo de datos y almacenar un único archivo en el disco duro. He manejado la información mediante objetos de R. En mi experiencia personal es una buena opción ya que limitas las posibilidades de que los datos se lean mal al importar/exportar y además suelen ocupar menos espacio en el disco duro que los archivos csv o txt. En todo caso, sería fácil modificar el código anterior cambiando la función *saveRDS* por *write.table*, *write.csv*, etc.

Generar un único objeto de R con la información descargada y almacenada de manera conjunta en una carpeta del PC. 

```{r}
files<-list.files(path="outputs/data/rgbif/descarga_ejemplo", full.names=TRUE) 
files
files<-stringr::str_subset(files, "_down.Rdata") #Selecciono datos con ocurrencias

my.df<-data.frame(NULL)
for (i in files) {
temp<-readRDS(i) 
my.df<-rbind(my.df,temp) #Columnas coinciden. Usar bind_rows(dplyr) si las columnas son distintas.
}
dim(my.df)
head(my.df)
```

## 4. Las funciones *occ_download* como alternativa para grandes descargas de datos


*rgbif* sólo puede descargar datos de especies con menos de 100.000 ocurrencias. Si alimentamos *occ_search()* con más datos obtendremos un error. Podemos hacer un bucle sencillo para comprobar si las especies de estudio superan los 100.000 datos.


```{r}
presencias<- as.vector(NULL)
for (i in splist){
tax.backbone<-name_backbone(name=i, rank='species', kingdom='plants')
key<-tax.backbone$usageKey
dat.search<- occ_count(taxonKey = key)
presencias <-c(presencias,dat.search)
}
```

Podemos visualizar los resultados y unirlos al listado de especies

```{r}
presencias<-data.frame(splist,presencias)
presencias
```


En los casos con más de 100.000 datos. Una forma de sortear esta limitación es aplicar un bucle que descargue los datos de manera secuencial. Por ejemplo, podemos decargar los datos de manera independiente para cada año, país de procedencia etc...

Sin embargo, la familia de funciones *occ_download()* son más apropiadas para solicitudes de datos más grandes. Los autores de rgbif aconsejan esta segunda opcion al considerarla más reproducible y más fácil una vez se aprende el funcionamiento.

La primera función importante es precisamente *occ_download()*  Con ella puedes especificar qué consulta quieres realizar.  Las interfaces de*occ_search()* y *occ_download()* son diferentes. Ten en cuenta que sólo puedes realizar 3 descargas simultáneas con *occ_download()*, así que planifica bien.

### 4.1 Registrarse en GBIF

Para poder utilizar occ_download tenemos que tener registrado en GBIF un nombre de usuario y una contraseña e email asociados.

[https://www.gbif.org/es/] (https://www.gbif.org/es/)

```{r,  include=FALSE}
user<-"carlos.lara.romero"
pwd<-"samgamyi69"
email<-"carlos.lara.romero@gmail.com"
```


```{r, eval=FALSE, messsage=FALSE}
user<-escribe usuario gbif
pwd<-escribe contraseña gbif
email<-escribe email gbif
```

### 4.2 Iniciar una descarga

En lugar de pasar parámetros como *hasCoordinate = TRUE* en *occ_search*, para las descargas construimos consultas utilizando cualquiera de las funciones *pred* (predicados). Esto permite realizar consultas mucho más complejas que las que se pueden hacer con *occ_search*.

Se pueden utilizar operadores distintos de **=** (igual a). Algunas de las funcioes **pred** más comunes. 

- pred: equals
- pred_lt: lessThan
- pred_lte: lessThanOrEquals
- pred_gt: greaterThan
- pred_gte: greaterThanOrEquals
- pred_not: not
- pred_like: like


Consulta  la documentación de los predicados aquí:  https://www.gbif.org/developer/occurrence#predicates

```{r}
res <- occ_download(pred("taxonKey", 5284702), pred("hasCoordinate", TRUE), #Cedrus Atlantica, Sólo coordenadas
                    user=user,
                    email=email,
                    pwd= pwd) 
res
```

Con occ_download se trabaja de manera similar a una descarga desde la interfaz web. *occ_download* envía la solicitud a GBIF, ellos tienen que prepararla primero, luego cuando está hecha puedes descargarla. 

Lo que *occ_download* devuelve son algunos metadatos útiles que informan sobre la descarga, y nos ayudan a comprobar y saber cuándo se ha realizado la descarga.

### Comprobar el estado de la descarga

Para comprobar el estado de la descarga podemos pasar el objeto generado por *occ_download*  a *occ_download_meta* 

```{r}
occ_download_meta(res)
```

Continua ejecutando occ_download_meta hasta que el valor del estado sea *SUCCEED* o *KILLED*. 

### 4.3 Obtención de datos

Cuando cambie a *SUCCEED* se puede descargar los datos con *occ_download_get*


```{r, include=FALSE}
Sys.sleep(360) # Suspender ejecución expresiones R durante intervalo de tiempo determinado.
```


```{r, warning=FALSE, message=FALSE}
path<-"outputs/data/rgbif/occ_download/"
dir.create(path)
down.key <- occ_download_meta(res)$key 
dat <- occ_download_get(key=down.key, path=path, overwrite = TRUE)
```

Para cargar los datos descargados en consola de R hay usar *occ_download_import*

```{r}
dat.imp<-occ_download_import(dat)
```


### 4.4 Citando los datos de las descargas

Usando la función *gbif_citation()* podemos obtener citas para nuestras descargas. Esta funcón proporciona  la citación global y las citas de cada conjunto de datos de manera independiente

```{r}
cit<-gbif_citation(dat)
cit$download #Citación global
length(cit$datasets) #157 conjunto de datos
cit$datasets [1] #Cita para el primer conjunto de datos
saveRDS(cit, paste0("outputs/data/rgbif/get/","dat2","_citation",".Rdata"))
```

### 4.5 Vectorizar búsquedas

Para descargar datos de varias especies necesitamos usar el predicado *pred_in*. Permite vectoriar y aplicar varios valores a una consulta


```{r}
gbif_taxon_keys<-c(2977832,2977901,2977966,2977835,2977863)

multiple<-occ_download(
  pred_in("taxonKey", gbif_taxon_keys), 
  format = "SIMPLE_CSV",
  user=user,pwd=pwd,email=email)

```


Para automatizar todo el proceso podemos crear un bucle para iterar mientras se está preparando nuetra solicitud. El bucle se detiene con éxito cuando se completa y *occ_download_meta* devuelve el resultado de "SUCCEED"

```{r}
still_running <- TRUE
status_ping <- 30 #seconds

while (still_running) {
  meta <- occ_download_meta(multiple)
  status <- meta$status
  print(status)
  still_running <- status %in% c("PREPARING", "RUNNING")
  Sys.sleep(status_ping) # Suspender ejecución expresiones R durante intervalo de tiempo determinado.
}

occ_download_meta(multiple)
down.key <- occ_download_meta(multiple)$key 
```

```{r, message=FALSE}
dat2<-occ_download_get(key=down.key, path=path,overwrite=TRUE)
dat2.imp<-occ_download_import(dat2)
unique(dat2.imp$scientificName)
unique(dat2.imp$taxonKey)
head(dat2.imp)
cit2<-gbif_citation(dat2)
dat2<-occ_download_import(dat2) # Import into R
dat2[sample(1:nrow(dat2),5),c("species","decimalLatitude","decimalLongitude","countryCode","issue")]
saveRDS(cit2, paste0(path,"dat2","_citation",".Rdata"))
```


## 5. Conjunto de datos derivados

GBIF tiene una herramienta para generar conjunto de datos derivados. Un conjunto de datos derivado es un registro citable (con un DOI único) que representa un conjunto de datos que no existe como una descarga convencional y no filtrada de GBIF.org.

https://www.gbif.org/es/derived-dataset]https://www.gbif.org/es/derived-dataset

A través del enalce se accede a la herramienta de GBIF para generar el DOI  a la que sólo necesitamos aportar: i) una lista de conjuntos de datos contribuyentes, ii) el recuento de registros de cada conjunto de daots y, en aras de la reproducibilidad, la url del repositorio público en el que se aloja el conjunto de datos derivado (figshare, dryad etc.). Los datos contribuyentes se pueden identificar con el campo datasetKey.

 En resumen, hay que generar un archivo de este tipo:

-  datasetKey1    25 records
-  datasetKey2    545 records
-  datasetKey3    200 records

GBIF recomienda el uso de conjuntos de datos derivados para citar los datos obtenidos mediante llamadas síncronas a la API como las utilizadas por rgbif en occ_data() y occ_search(). Pero aunque se utilice la función occ_download() para generar una única descarga de datos es posible que durante el proceso de limpieza y filtrado se eliminen registros de conjunto de datos completos (datasets). Por ello, es una herramienta muy útil para generar DOIs específicos para nuestros datos que permitan citar exactamente los datos utilizados.

## 6. Limpieza de datos

Los datos proporcionados por agregadores de grandes datos como GBIF son muy valiosos para la investigación. Sin embargo, existen algunos problemas relacionados con la calidad de los datos, sobre todo porque estos datos se componen de una variedad de métodos de recolección diferentes, de diferentes fuentes  y son digitalizados y editados por varias personas y algoritmos en diferentes momentos en el tiempo y el espacio.


Durante el proceso de indexación sobre los datos brutos, GBIF añade problemas y banderas a los registros con problemas comunes de calidad de datos. Esta información queda almacenada en el campo *issues*.

[https://data-blog.gbif.org/post/issues-and-flags/]https://data-blog.gbif.org/post/issues-and-flags/


```{r}
unique(cedrus$data$issues)
```

 Con la función *gbif_issues()* se pueden consultar todos los tipos de problemas
```{r}
head(gbif_issues())
```

Filtrar datos por este campo es sencillo:

```{r}
cedrus2<-cedrus$data[cedrus$data$issues!="bri",] #Elminamos ocurrencias sin origen del dato preciso
dim(cedrus2)
```


Otro campo importante es *basisOfRecords* ya que nos permite filtra por la fuente de datos

```{r}
table(cedrus2$basisOfRecord)
```

```{r}
cedrus2<-cedrus2[cedrus2$basisOfRecord=="HUMAN_OBSERVATION",]
table(cedrus2$basisOfRecord)
```


Las posibilidades de filtrado de los datos son muy numerosas. Entrar en detalle en este proceso se escapa del objetivo de este seminario. En todo caso, existen algunos paquetes en R que nos proporcionan una línea de trabajo sobre cómo limpiar los registros de ocurrencia recuperados de GBIF (o de cualquier otra base de datos): *scrubr* (https://github.com/ropensci/scrubr), *biogeo* (https://markrobertson.co.za/biogeo/) o *CoordinateCleaner* (https://ropensci.github.io/CoordinateCleaner/articles/Cleaning_GBIF_data_with_CoordinateCleaner.html)

Los paquetes  *scrubr* y *taxize* (https://github.com/ropensci/taxize) son dos muy buenas alernativas para la limpieza taxonómica

